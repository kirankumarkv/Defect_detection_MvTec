{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5804d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b34be5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotDefectClassifier(nn.Module):\n",
    "    def __init__(self, num_attributes, num_classes):\n",
    "        super(ZeroShotDefectClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # feature extraction layers\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))  # Adjust output size\n",
    "        self.classifier = nn.Linear(256 * 4 * 4, num_attributes)  # Adjust input size\n",
    "        self.fc = nn.Linear(num_attributes, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b73c8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "num_attributes = 10  \n",
    "num_classes = 2  # Number of classes (good or broken)\n",
    "model = ZeroShotDefectClassifier(num_attributes, num_classes)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c60b1b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and dataloader for training data\n",
    "batch_size = 32\n",
    "data_dir = 'D:/MvTec/mvtec_anomaly_detection'\n",
    "class_name = 'bottle'\n",
    "class_dir = os.path.join(data_dir, class_name)\n",
    "train_dir = os.path.join(class_dir, 'Train')\n",
    "product1_train_dataset = ImageFolder(train_dir, transform=transform)\n",
    "product1_train_loader = DataLoader(product1_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class_name = 'carpet'\n",
    "class_dir = os.path.join(data_dir, class_name)\n",
    "train_dir = os.path.join(class_dir, 'Train')\n",
    "product2_train_dataset = ImageFolder(train_dir, transform=transform)\n",
    "product2_train_loader = DataLoader(product2_train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "79825cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data loaders \n",
    "train_datasets = [product1_train_dataset, product2_train_dataset]  # Add more datasets as needed\n",
    "combined_train_dataset = ConcatDataset(train_datasets)\n",
    "combined_train_loader = DataLoader(combined_train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8ac52cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: bottle, Epoch [1/10], Loss: 0.6726\n",
      "Class: bottle, Epoch [2/10], Loss: 0.0464\n",
      "Class: bottle, Epoch [3/10], Loss: 0.0000\n",
      "Class: bottle, Epoch [4/10], Loss: 0.0000\n",
      "Class: bottle, Epoch [5/10], Loss: 0.0000\n",
      "Class: bottle, Epoch [6/10], Loss: 0.0000\n",
      "Class: bottle, Epoch [7/10], Loss: 0.0000\n",
      "Class: bottle, Epoch [8/10], Loss: 0.0000\n",
      "Class: bottle, Epoch [9/10], Loss: 0.0000\n",
      "Class: bottle, Epoch [10/10], Loss: 0.0000\n",
      "Class: cable, Epoch [1/10], Loss: 0.5829\n",
      "Class: cable, Epoch [2/10], Loss: 0.1080\n",
      "Class: cable, Epoch [3/10], Loss: 0.0000\n",
      "Class: cable, Epoch [4/10], Loss: 0.0000\n",
      "Class: cable, Epoch [5/10], Loss: 0.0000\n",
      "Class: cable, Epoch [6/10], Loss: 0.0000\n",
      "Class: cable, Epoch [7/10], Loss: 0.0000\n",
      "Class: cable, Epoch [8/10], Loss: 0.0000\n",
      "Class: cable, Epoch [9/10], Loss: 0.0000\n",
      "Class: cable, Epoch [10/10], Loss: 0.0000\n",
      "Class: carpet, Epoch [1/10], Loss: 0.4640\n",
      "Class: carpet, Epoch [2/10], Loss: 0.0836\n",
      "Class: carpet, Epoch [3/10], Loss: 0.0002\n",
      "Class: carpet, Epoch [4/10], Loss: 0.0000\n",
      "Class: carpet, Epoch [5/10], Loss: 0.0000\n",
      "Class: carpet, Epoch [6/10], Loss: 0.0000\n",
      "Class: carpet, Epoch [7/10], Loss: 0.0000\n",
      "Class: carpet, Epoch [8/10], Loss: 0.0000\n",
      "Class: carpet, Epoch [9/10], Loss: 0.0000\n",
      "Class: carpet, Epoch [10/10], Loss: 0.0000\n",
      "Class: capsule, Epoch [1/10], Loss: 0.2392\n",
      "Class: capsule, Epoch [2/10], Loss: 0.0000\n",
      "Class: capsule, Epoch [3/10], Loss: 0.0000\n",
      "Class: capsule, Epoch [4/10], Loss: 0.0000\n",
      "Class: capsule, Epoch [5/10], Loss: 0.0000\n",
      "Class: capsule, Epoch [6/10], Loss: 0.0000\n",
      "Class: capsule, Epoch [7/10], Loss: 0.0000\n",
      "Class: capsule, Epoch [8/10], Loss: 0.0000\n",
      "Class: capsule, Epoch [9/10], Loss: 0.0000\n",
      "Class: capsule, Epoch [10/10], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Define the directory paths for other classes\n",
    "train_dirs = [\n",
    "    os.path.join(dataset_root, \"cable\", \"train\"),\n",
    "    os.path.join(dataset_root, \"carpet\", \"train\"),\n",
    "    os.path.join(dataset_root, \"capsule\", \"train\"),\n",
    "    os.path.join(dataset_root, \"hazelnut\", \"train\")\n",
    "]\n",
    "\n",
    "# Initialize the model for each class\n",
    "models = []\n",
    "for _ in range(len(classes)):\n",
    "    model = ZeroShotDefectClassifier(num_attributes, num_classes)\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "\n",
    "# Training loop for each class\n",
    "for class_index, train_dir in enumerate(train_dirs):\n",
    "    train_dataset = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = models[class_index]\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Class: {classes[class_index]}, Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ab31b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2417\n",
      "Precision by Class:\n",
      " - bottle:\n",
      "   - broken_large: 1.0000\n",
      "   - broken_small: 0.0000\n",
      "   - contamination: 0.0000\n",
      " - carpet:\n",
      "   - color: 1.0000\n",
      "   - cut: 0.0000\n",
      "   - hole: 0.0000\n",
      "   - metal_contamination: 0.0000\n",
      "   - thread: 0.0000\n",
      " - capsule:\n",
      "   - crack: 1.0000\n",
      "   - poke: 0.0000\n",
      "   - scratch: 0.0000\n",
      "   - faulty_imprint: 0.0000\n",
      "   - squeeze: 0.0000\n",
      " - hazelnut:\n",
      "   - cut: 0.0000\n",
      "   - crack: 1.0000\n",
      "   - hole: 0.0000\n",
      "   - print: 0.0000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "class_labels = ['bottle', 'carpet', 'capsule', 'hazelnut']\n",
    "defect_labels = {\n",
    "    'bottle': ['broken_large', 'broken_small', 'contamination'],\n",
    "    'carpet': ['color', 'cut', 'hole','metal_contamination','thread'],\n",
    "    'capsule': ['crack', 'poke', 'scratch','faulty_imprint','squeeze' ],\n",
    "    'hazelnut': ['cut', 'crack', 'hole','print']\n",
    "}\n",
    "\n",
    "# Initialize variables for counting correct predictions\n",
    "correct_predictions_by_class = {class_label: {defect_label: 0 for defect_label in defect_labels[class_label]} for class_label in class_labels}\n",
    "total_samples_by_class = {class_label: {defect_label: 0 for defect_label in defect_labels[class_label]} for class_label in class_labels}\n",
    "\n",
    "# Iterate over the test data and labels for each class\n",
    "for class_label in class_labels:\n",
    "    test_dir = os.path.join(data_dir, class_label, \"test\")\n",
    "    test_dataset = ImageFolder(test_dir, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Count correct predictions for defect classes\n",
    "        for defect_label in defect_labels[class_label]:\n",
    "            defect_samples = (labels == test_dataset.class_to_idx[defect_label]).sum().item()\n",
    "            correct_predictions_by_class[class_label][defect_label] += ((predicted == labels) & (labels == test_dataset.class_to_idx[defect_label])).sum().item()\n",
    "            total_samples_by_class[class_label][defect_label] += defect_samples\n",
    "\n",
    "# Calculate precision for defect classes\n",
    "precision_by_class = {}\n",
    "for class_label in class_labels:\n",
    "    precision_by_class[class_label] = {}\n",
    "    for defect_label in defect_labels[class_label]:\n",
    "        precision_by_class[class_label][defect_label] = correct_predictions_by_class[class_label][defect_label] / total_samples_by_class[class_label][defect_label]\n",
    "\n",
    "# Calculate overall accuracy and precision\n",
    "total_samples = sum([sum(total_samples_by_class[class_label].values()) for class_label in class_labels])\n",
    "correct_predictions = sum([sum(correct_predictions_by_class[class_label].values()) for class_label in class_labels])\n",
    "accuracy = correct_predictions / total_samples\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Precision by Class:\")\n",
    "for class_label in class_labels:\n",
    "    print(f\" - {class_label}:\")\n",
    "    for defect_label in defect_labels[class_label]:\n",
    "        print(f\"   - {defect_label}: {precision_by_class[class_label][defect_label]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
